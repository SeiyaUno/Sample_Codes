{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55cfeb60",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3248a",
   "metadata": {},
   "source": [
    "We first began by getting a sense of the data by using the Pandas display() function which provides a nice table with a couple rows of data. We can see all of the labels used in the dataset from here where we also noticed that a good portion of the data is not numerical but categorical which was something we had to consider later on in our thought process. We also saw that a relatively significant amount of the data was missing values, around 13%, which was another thing we had to handle later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c9371",
   "metadata": {},
   "source": [
    "Through various bar charts where we compare the categories or ranges of a feature against the income levels and histograms of features we found patterns for certain features that were useful. We noticed that two features Capital Gain and Capital Loss all were essentially 0's except for very little samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8dde93",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1794d7",
   "metadata": {},
   "source": [
    "## Trees\n",
    "We decided to use decision trees for this dataset since we observed that the dataset contains various categorical and continuous variables that can be split."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Improve performance using Adaboost\n",
    "\n",
    "For this section, we tried decision trees with Adaboost, which trains multiple weak classifiers on weighted data to make correct predictions. Each classifier will make different predictions since they are trained on data with different importance. Thus, when all the trained classifiers are aggregated together, the model will be able to make correct predictions.\n",
    "We compare the performance of Adaboost models against the baseline decision tree model to measure how much it improves the performance. We use unscaled data for training Adaboost models since base decision trees are not affected by the scale of features in the data. The result of several models with different parameters and settings are shown in the table below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Table image](table_img.png \"Table 1\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "ad136998",
   "metadata": {},
   "source": [
    "**Explanations of each experiment**\n",
    "We explored models with different parameters to find the best-performing model for the dataset we are working with.\n",
    "\n",
    "- Ada1 (estimator = decision tree with depth100, n_estimators = 100, learing_rate = 1)\n",
    "This model does not perform much better than the baseline tree. This is because the base estimator is the same as the baseline tree. It means that each classifier in the Adaboost model learns data well enough in the same way, thus it does not make major differences when aggregated.\n",
    "- Ada2 (estimator = decision tree with depth1, n_estimators = 100, learing_rate = 1)\n",
    "The validation error improves by about 5 % compared to the baseline.\n",
    "- Ada3 (estimator =decision tree with different depth, n_estimators = 100, learing_rate = 1)\n",
    "For this one, we trained multiple models with decision trees with different depths (1,2,3,4,5,10,20) as a base classifier for Adaboost. The intent was to measure how simple the base classifier should be. After depth2, the model started to overfit. The best score from this experiment was the tree with depth2, which is shown in the table above.\n",
    "- Ada4  (estimator =decision tree with depth 1 or 2, n_estimators = varies, learing_rate = 1)\n",
    "This experiment tested models with different numbers of estimators (50, 100, 200, 500, 1000). For this one, we used decision trees with depths 1 and 2 since we thought that depth2 could make the model overfit faster than depth1 because of the relative complexity. It turns out that the model with the depth2 tree starts to overfit after 100 estimators. The model with depth1 seems to get better as we increase the number of estimators. However, we did not test it with more than 1000 estimators because the rate of improvement was decreasing as we increased the number of estimators. The improvement we would get for any more estimators will be very small.\n",
    "- Ada 5  (estimator =decision tree with depth 1, n_estimators = 1000, learing_rate = varies)\n",
    "We tested models with different learning rates (2, 1, 0.1, 0.01, 0.001) to see how it affects model performance. The model with learning rate = 1 achieved the best performance. It starts to overfit when we use a learning rate lower than 1.\n",
    "\n",
    "Overall, for the dataset we chose, decision trees with Adaboost work best when we use decision stumps as a base estimator, number of estimator = 1000, and learning rate = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ddf02",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92188238",
   "metadata": {},
   "source": [
    "A point was made in class that very often stacking can result in greater accuracy than the individuals models that are used in the ensemble. In order to verify that point we chose to also try stacking with various models and see if that worked for our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39594c03",
   "metadata": {},
   "source": [
    "Although it might not be necesary we chose to first at least give the indiviudal models a fightning chance and tried to optimize them first by tuning. The ensembles consists of two KNN predictors, a LogisticRegression classifier, a NearestCentroid classifier, a SVM, and a DecisionTree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0260c3f",
   "metadata": {},
   "source": [
    "The KNN predictors were optimized by minimizing testing error for various K-Values. The optimal range seemed to be between 35-45 after many iterations so the two best predictors were included in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2406419d",
   "metadata": {},
   "source": [
    "The SVM was optimized with various parameter values using GridSearchCV with different kernels and regularization parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84512604",
   "metadata": {},
   "source": [
    "None of the models alone were able to get under 15% testing error however when using stacking they were able to achieve a testing error of 14.9% which is not a significant improvement but it does show they performed better together than alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
